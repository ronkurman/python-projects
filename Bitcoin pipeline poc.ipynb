{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This notebook is a POC of a Bitcoin pipeline of data ingest and training**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we will get the workspace we are working at which is by import Workspace module from azureml.core and using from_config()"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws=Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we will want to attach our compute for the pipeline by importing the module ComputeTraget and AmlCompute.\n",
        "first we check if we have the desired compute in our workspace and if not we create a new one"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget,AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name='ronku1'\n",
        "try:\n",
        "    pipeline_cluster=ComputeTarget(workspace=ws,name=cluster_name)\n",
        "    print('Found')\n",
        "except ComputeTargetException:\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', max_nodes=2)\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we need to create the environment of the pipeline(the vm behind) and here we are using docker to enable faster compute with our GPU.\n",
        "Or we can use an existing environment that we have"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import RunConfiguration,DockerConfiguration\n",
        "location='/mnt/batch/tasks/shared/LS_root/mounts/clusters/ronku1/code/Users/ronku/bitcoin_pipeline/' #location of the pipeline folder in the azureml\n",
        "#create a python environment for the experiment\n",
        "experiment_env=Environment.from_conda_specification('bitcoin_env',location+'/bitcoin_env.yml')\n",
        "# Specify a GPU base image\n",
        "#mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\n",
        "#mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04\n",
        "#mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04\n",
        "experiment_env.docker.base_image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04\"\n",
        "\n",
        "experiment_env.register(workspace=ws,)\n",
        "#untill here we register the environment and we can skip and comment all the lines above if we have existing and just use the line below\n",
        "registered_env=Environment.get(ws,'bitcoin_env')\n",
        "docker_config=DockerConfiguration(use_docker=True)#set docker for true to using the docker.base_image\n",
        "#Create the pipeline config\n",
        "pipeline_run_config=RunConfiguration()\n",
        "\n",
        "#using the compute for the pipeline\n",
        "pipeline_run_config.target=pipeline_cluster\n",
        "\n",
        "#assigning the env\n",
        "pipeline_run_config.environment=registered_env#define the environment\n",
        "pipeline_run_config.docker=docker_config#enable docker usage in the pipeline config\n",
        "print('pipeline config created')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile\n",
        "name: bitcoin_env\n",
        "channels:\n",
        "- conda-forge\n",
        "dependencies: \n",
        "- python==3.8.10\n",
        "- scikit-learn\n",
        "- ipykernel\n",
        "- matplotlib\n",
        "- pandas\n",
        "- pip\n",
        "- tensorflow=\n",
        "- keras\n",
        "- pip: \n",
        "  - azureml-defaults\n",
        "  - pyarrow\n",
        "  - tensorflow\n",
        "  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we are starting to create the steps in the pipeline here i used one for data ingest, some preprocessing and predicting while ingesting and one for the training step because i wanted each one to have different schedules"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "below data ingest step"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##creating the pipeline\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "ingest_step=PythonScriptStep(name='Data Ingest',\n",
        "                            source_directory=location,\n",
        "                            script_name='data_ingest.py',\n",
        "                            compute_target=pipeline_cluster,\n",
        "                            runconfig=pipeline_run_config,\n",
        "                            allow_reuse=True)\n",
        "\n",
        "\n",
        "print('Pipeline steps ready')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after the %%writefile you enter the location the file to be written with $ at the start"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import requests\n",
        "from azureml.core import Run\n",
        "import os\n",
        "import time\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras import layers\n",
        "run=Run.get_context()\n",
        "\n",
        "#getting the data from gateio API\n",
        "def get_data(start,end):\n",
        "    host = \"https://api.gateio.ws\"\n",
        "    prefix = \"/api/v4\"\n",
        "    headers = {'Accept': 'application/json', 'Content-Type': 'application/json'}\n",
        "\n",
        "    url = '/spot/candlesticks'\n",
        "    query_param = f'currency_pair=BTC_USDT&interval=15m&from={str(start)}&to={end}'\n",
        "    r = requests.request('GET', host + prefix + url + \"?\" + query_param, headers=headers)\n",
        "    #checking if the response is not empty\n",
        "    if str(r.json())!='200':\n",
        "        return '-1'\n",
        "    else:\n",
        "        return(r.json()[-1])\n",
        "#setting the current time and convert it to timestamp\n",
        "current=int(datetime.datetime.now().timestamp())\n",
        "filename='Bitcoin_days.csv'\n",
        "preped_name='preped_data.csv'\n",
        "prediction_name='prediction.csv'\n",
        "location='/mnt/batch/tasks/shared/LS_root/mounts/clusters/ronku1/code/Users/ronku/bitcoin_pipeline/'\n",
        "#getting our dataset from the folder\n",
        "temp_dataset=pd.read_csv(location+filename,index_col=0)\n",
        "#start time is the last timestamp in the dataset\n",
        "start=temp_dataset.iloc[-1,1]\n",
        "#moving in jumps of 900 becuase we are using 15 mins intervals\n",
        "end=start+900\n",
        "#updating a predictions files with a timestamp\n",
        "predictions=pd.read_csv(location+'prediction.csv',index_col=0)\n",
        "\n",
        "#loading the model that we trained\n",
        "model=tensorflow.keras.models.load_model(location+'outputs/bitcoin-pred-model')\n",
        "#empty numpy array for the new predictions\n",
        "temp_pred=np.zeros((1,2))\n",
        "#checking that we don't go over the current time\n",
        "if end<=current:\n",
        "    #getting our dataset from the folder\n",
        "    dataset=pd.read_csv(filename,index_col=0)\n",
        "    #getting our the preped dataset from the folder\n",
        "    preped=pd.read_csv(preped_name,index_col=0)\n",
        "    while end<=current:\n",
        "        #getting new data point and convert to numpy array\n",
        "        new_data=get_data(int(start),int(end))\n",
        "        new_data=np.array(new_data)\n",
        "        \n",
        "         if new_data=='-1':\n",
        "            #if the response is empty copy the last row\n",
        "            new_data=dataset[-1,:]\n",
        "        else:\n",
        "            new_data=np.array(new_data)\n",
        "        #keeping on variable unchanged for later use\n",
        "        new_data1=np.reshape(new_data,(1,-1))\n",
        "        #appending new row to the dataset\n",
        "        new_data=np.reshape(new_data,(1,-1))\n",
        "        dataset=np.array(dataset)\n",
        "        dataset=np.concatenate((dataset,new_data),axis=0)\n",
        "        #dividing the last column to get the rate of change\n",
        "        temp_data=float(dataset[-1,2])/float(dataset[-2,2])\n",
        "        #preprocessing the new point to predict the value\n",
        "        temp_data1=np.reshape(np.array(temp_data),(1,1))\n",
        "        temp_data1=np.concatenate((new_data1[:,1:],temp_data1),axis=1)\n",
        "        temp_data1=np.reshape(temp_data1,(1,1,6)).astype('float64')\n",
        "        #predicting the new outcome\n",
        "        pred=(model.predict(temp_data1))\n",
        "        pred =1 if pred>=0.5 else 0\n",
        "        pred=np.array([start,pred])\n",
        "        pred=np.reshape(pred,(1,-1))\n",
        "        #updating the temp_pred dataset\n",
        "        temp_pred=np.concatenate((temp_pred,pred),axis=0)\n",
        "        \n",
        "        #setting the the target to be 1 if we get that the rate of change is greater than 1\n",
        "        target=1 if temp_data>=1 else 0\n",
        "        temp_data=np.reshape(np.array([temp_data,target]),(1,-1))\n",
        "\n",
        "        second_last_row=np.reshape(np.array(dataset[-2,:]),(1,-1))\n",
        "        second_last_row=np.concatenate((second_last_row,temp_data),axis=1)\n",
        "        #updating the preped dataset\n",
        "        preped=np.concatenate((np.array(preped),second_last_row),axis=0)\n",
        "        start+=900\n",
        "        end+=900\n",
        "    \n",
        "\n",
        "    #updating the final variables before saving as csv files\n",
        "    predictions=np.array(predictions)\n",
        "    predictions=np.concatenate((predictions,temp_pred[1:,:]),axis=0)\n",
        "    predictions=pd.DataFrame(predictions)\n",
        "\n",
        "    dataset=pd.DataFrame(dataset)\n",
        "    preped=pd.DataFrame(preped)\n",
        "\n",
        "    predictions.to_csv(location+prediction_name)\n",
        "    dataset.to_csv(location+filename)\n",
        "    preped.to_csv(location+preped_name)\n",
        "    print('loading completed')\n",
        "else:\n",
        "    print(f'current time {current} is greater than {end}')\n",
        "run.complete()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "below training step"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##creating the pipeline\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "train_step=PythonScriptStep(name='Train_step',\n",
        "                            source_directory=location,\n",
        "                            script_name='train.py',\n",
        "                            compute_target=pipeline_cluster,\n",
        "                            runconfig=pipeline_run_config,\n",
        "                            allow_reuse=True)\n",
        "\n",
        "\n",
        "print('Pipeline steps ready')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import argparse\n",
        "import joblib\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras import layers\n",
        "from azureml.core import Run,Model\n",
        "from azureml.core.experiment import Experiment\n",
        "import os\n",
        "location='/mnt/batch/tasks/shared/LS_root/mounts/clusters/ronku1/code/Users/ronku/bitcoin_pipeline/'\n",
        "\n",
        "\n",
        "run = Run.get_context()\n",
        "\n",
        "print('Loading Data...')\n",
        "data=pd.read_csv(location+'preped_data.csv',index_col=0)\n",
        "length=data.shape[0]\n",
        "data=np.array(data)[:,1:]\n",
        "train_len=int(0.8*length)\n",
        "#scaling the data to the range of 0 to 1\n",
        "sc=MinMaxScaler(feature_range=(0,1))\n",
        "sc.fit(data[:train_len,:])\n",
        "#applying the transform to the whole data\n",
        "data=sc.transform(data[:,:])\n",
        "#data generator for the LSTM model\n",
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "    shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "\n",
        "    if shuffle:\n",
        "        rows = np.random.randint(\n",
        "            min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "        if i + batch_size >= max_index:\n",
        "            i = min_index + lookback\n",
        "        rows = np.arange(i, min(i + batch_size, max_index))\n",
        "        i += len(rows)\n",
        "    samples = np.zeros((len(rows),\n",
        "            lookback // step,\n",
        "            data.shape[-1]-1))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "        indices = range(rows[j] - lookback, rows[j], step)\n",
        "        samples[j] = data[indices,:-1]\n",
        "        targets[j] = data[rows[j] + delay][-1]\n",
        "    return (samples, targets)\n",
        "lookback=96*14 #observations will go back 14 day\n",
        "step=1 #observations will be sampled at one data point per hour\n",
        "delay=4 #target will be hour in the future\n",
        "batch_size=1024*3\n",
        "#getting train and val sets\n",
        "x_train,y_train = generator(data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=0,\n",
        "                    max_index=int(0.8*length),\n",
        "                    shuffle=True,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "x_val,y_val = generator(data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=int(0.8*length)+1,\n",
        "                    max_index=None,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "val_steps=int(0.9*length-0.8*length-lookback)\n",
        "#model creation\n",
        "model=tensorflow.keras.models.Sequential()\n",
        "model.add(tensorflow.keras.layers.LSTM(512,return_sequences=True,input_shape=(None,data.shape[-1]-1)))\n",
        "model.add(tensorflow.keras.layers.LSTM(256,return_sequences=True))\n",
        "model.add(tensorflow.keras.layers.LSTM(128,return_sequences=False))\n",
        "model.add(tensorflow.keras.layers.Dense(32,activation='relu',kernel_regularizer='l2'))\n",
        "model.add(tensorflow.keras.layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "adam=tensorflow.keras.optimizers.Adam()\n",
        "#compiling with adam and using AUC and accuracy for the metrics\n",
        "model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy','AUC'])\n",
        "model.summary()\n",
        "#training the model and saving to new variable\n",
        "history=model.fit(x=x_train,y=y_train,epochs=1,validation_data=(x_val,y_val),verbose=1)\n",
        "#plotting and logging graphs of thew accuracy auc and loss to azureml studio\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "val_auc=history.history['val_auc']\n",
        "train_auc=history.history['auc']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "current=int(datetime.datetime.now().timestamp())\n",
        "\n",
        "\n",
        "plt.plot(epochs, acc,label='Training acc')\n",
        "plt.plot(epochs, val_acc, label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "run.log_image(name='accuracy'+str(current),plot=plt)\n",
        "print('accuracy plot was logged')\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "run.log_image(name='loss'+str(current),plot=plt)\n",
        "print('loss plot was logged')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, train_auc, label='Training AUC')\n",
        "plt.plot(epochs, val_auc, label='Validation AUC')\n",
        "plt.title('Training and validation AUC')\n",
        "plt.legend()\n",
        "run.log_image('AUC'+str(current),plot=plt)\n",
        "print('AUC plot was logged')\n",
        "val_auc=float(val_auc[-1])\n",
        "val_acc=float(val_acc[-1])\n",
        "#logging auc and accuracy metrics for later use\n",
        "run.log(name='AUC',value=val_auc)\n",
        "run.log(name='Accuracy',value=val_acc)\n",
        "#loading the current model\n",
        "old_model=Model(workspace=run.experiment.workspace,name='bitcoin-pred')\n",
        "run_id=old_model.properties['ID']\n",
        "exp=Experiment(workspace=run.experiment.workspace,name='Bitcoin-Pipeline')\n",
        "\n",
        "old_run_metrics=Run(experiment=exp,run_id=run_id).get_metrics()\n",
        "old_auc=old_run_metrics['AUC']\n",
        "old_acc=old_run_metrics['Accuracy']\n",
        "#checking if new model is better than current model if so we will register the new one\n",
        "if val_auc>float(old_auc) or val_acc>float(old_acc):\n",
        "    print('Regestring model')\n",
        "    print(\"Saving model...\")\n",
        "    model.save(location+'outputs/bitcoin-pred-model')\n",
        "    model_file = location+'outputs/bitcoin-pred-model'\n",
        "    #keeping the auc accuracy in the properties of the model\n",
        "    Model.register(workspace=run.experiment.workspace,\n",
        "                    model_path=model_file,\n",
        "                    model_name='bitcoin-pred',\n",
        "                    tags={'training-context':'pipline'},\n",
        "                    properties={'ID':str(run.id),\n",
        "                    'AUC':str(val_auc),\n",
        "                    'Accuracy':str(val_acc)},\n",
        "                    run_id=str(run.id)\n",
        "else:\n",
        "    print('New model did not registered')\n",
        "run.complete()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we run the pipeline to check that everything is running properlly "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "#creating the pipeline\n",
        "pipeline_steps=[ingest_step]#pipeline_steps=[train_step]\n",
        "pipeline=Pipeline(workspace=ws,steps=pipeline_steps)\n",
        "\n",
        "#create the experiment and run the pipeline\n",
        "experiment=Experiment(workspace=ws,name='Bitcoin-Pipeline')\n",
        "pipeline_run=experiment.submit(pipeline,regenrate_outputs=True)\n",
        "print('Pipeline is submitted and ready for use')\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after the pipeline ran sucessfully we can publish an endpoint(for example in our exmaple a training endpoint)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Publishing the pipeline\n",
        "#we give the publish pipeline name and some description\n",
        "published_pipeline=pipeline_run.publish_pipeline(name='hourly-bitcoin-data-ingest-and-predict',description='Hourly data ingest',version='1.0')\n",
        "print(published_pipeline)\n",
        "#Endpoint\n",
        "rest_endpoint=published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can schedule the pipeline to run different time intervals for our uses.Notice you must publish a pipeline first and then you can schedule it.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence,Schedule,TimeZone\n",
        "\n",
        "#Running the pipeline everyday at midnight\n",
        "recurrence=ScheduleRecurrence(frequency='Hour',interval=1,start_time='2022-03-08T11:10:00',time_zone=TimeZone.IsraelStandardTime)\n",
        "daily_schedule=Schedule.create(ws,'hourly-bitcoin-data-ingest-and-predict',pipeline_id=published_pipeline.id,experiment_name='Bitcoin-Pipeline',recurrence=recurrence)\n",
        "print('Pipeline scheduled')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence,Schedule,TimeZone\n",
        "\n",
        "#Running the pipeline everyday at midnight\n",
        "recurrence=ScheduleRecurrence(frequency='Day',interval=1,start_time='2022-03-08T11:10:00',time_zone=TimeZone.IsraelStandardTime)\n",
        "daily_schedule=Schedule.create(ws,'Daily training',pipeline_id=published_pipeline.id,experiment_name='Bitcoin-Pipeline',recurrence=recurrence)\n",
        "print('Pipeline scheduled')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and we can cancel the schedules if we don't need some of them anymore"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedules = Schedule.list(ws)\n",
        "for sch in schedules:\n",
        "    Schedule.disable(sch)\n",
        "schedules"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}